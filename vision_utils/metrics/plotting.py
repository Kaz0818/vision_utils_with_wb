from __future__ import annotations
from pathlib import Path
from typing import List, TYPE_CHECKING, Any
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report
from tqdm import tqdm
import numpy as np  # â˜… è¿½åŠ ï¼šå‹ã‚†ã‚‰ãå¯¾ç­–ã§ä½¿ç”¨

# wandb ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ä¾å­˜ã«ã™ã‚‹
try:
    import wandb as _wandb  # å®Ÿè¡Œæ™‚ã«ä½¿ç”¨
    WANDB_AVAILABLE = True
except Exception:
    _wandb = None  # type: ignore
    WANDB_AVAILABLE = False

if TYPE_CHECKING:  # å‹ãƒã‚§ãƒƒã‚¯æ™‚ã®ã¿æ­£ç¢ºãªå‹ã‚’ä½¿ã†
    import wandb

class Visualizer:
    def __init__(self, wandb_run: "wandb.wandb_sdk.wandb_run.Run", writer: Any | None = None):
        self.writer = writer
        
        if not wandb_run:
            raise ValueError("wandb.Run object must be provided.")
        if not WANDB_AVAILABLE:
            raise ImportError("wandb ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ Visualizer ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚'pip install wandb' ã¾ãŸã¯ç’°å¢ƒå¤‰æ•° WANDB_DISABLED=true ã‚’ã”æ¤œè¨ãã ã•ã„ã€‚")
        self.wandb_run = wandb_run
        
        # wandbã®å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã«æˆæœç‰©ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.results_dir = Path(self.wandb_run.dir) / "results"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        print(f"[Visualizer] Artifacts will be saved to: {self.results_dir}")
    
    def run_evaluation(
        self,
        model: nn.Module,
        model_path: Path,
        dataloader: DataLoader,
        class_names: List[str],
        device: torch.device,
    ) -> None:
        """ 
        ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã€ãƒ—ãƒ­ãƒƒãƒˆã€W&Bã¸ã®ãƒ­ã‚°è¨˜éŒ²ã‚’ã™ã¹ã¦å®Ÿè¡Œã™ã‚‹
        """
        print("\n--- [Visualizer] Starting Final Evaluation ---")
        
        # 1. ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã‚€
        if model_path and model_path.exists():
            checkpoint = torch.load(model_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(device)
            model.eval()
            print(f"[Visualizer] Loaded model checkpoint from: {model_path}")
        elif model_path is None:
            model.to(device)
            model.eval()
        else:
            print(f"[Visualizer] ERROR: Model path not found: {model_path}")
            return
        
        # 2. äºˆæ¸¬ã¨æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’åé›†
        all_preds, all_labels = [], []
        with torch.no_grad():
            for inputs, labels in tqdm(dataloader, desc="[Visualizer] Predicting"):
                inputs = inputs.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                # â˜… numpyé…åˆ—ã‚’ãƒªã‚¹ãƒˆåŒ–ã—ã¦å‹ã®ã‚ºãƒ¬ã‚’æœ€å°åŒ–
                all_preds.extend(predicted.cpu().numpy().tolist())
                all_labels.extend(labels.cpu().numpy().tolist())
        
        # 3. Confusion Matrixã¨Classification Reportã‚’ç”Ÿæˆãƒ»ä¿å­˜ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        self.log_confusion_matrix(all_labels, all_preds, class_names)
        self.log_classification_report(all_labels, all_preds, class_names)
        
        print("ğŸš€ [Visualizer] Evaluation and plotting complete.")
        
    def log_confusion_matrix(self, y_true: List[int], y_pred: List[int], class_names: List[str]):
        """Confusion Matrixã‚’ä½œæˆã—ã€W&Bã«ãƒ­ã‚°ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹"""
        K = len(class_names)
        labels_idx = list(range(K))
        # â˜… é‡è¦ï¼šå…¨ã‚¯ãƒ©ã‚¹å›ºå®šã§è¡Œåˆ—ã‚µã‚¤ã‚ºã‚’ KÃ—K ã«ã™ã‚‹
        cm = confusion_matrix(y_true, y_pred, labels=labels_idx)
        cm_path = self.results_dir / "confusion_matrix.png"
        
        df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
        plt.figure(figsize=(12, 9))
        sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')
        plt.title("Confusion Matrix")
        plt.ylabel('Actual')
        plt.xlabel('Predicted')
        plt.tight_layout()
        plt.savefig(cm_path)
        plt.close()
        
        # W&Bã«ç”»åƒã‚’ãƒ­ã‚°ã¨ã—ã¦è¨˜éŒ²
        if WANDB_AVAILABLE and _wandb is not None:
            self.wandb_run.log({"evaluation/confusion_matrix": _wandb.Image(str(cm_path))})
        # ãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã‚‚ä¿å­˜
        self.wandb_run.save(str(cm_path), base_path=self.wandb_run.dir)
        print("[Visualizer] Logged confusion matrix to W&B.")
        
        
    def log_classification_report(self, y_true: List[int], y_pred: List[int], class_names: List[str]):
        """Classification Reportã‚’ä½œæˆã—ã€W&Bã«ãƒ­ã‚°ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹"""
        K = len(class_names)
        labels_idx = list(range(K))
        # â˜… é‡è¦ï¼šCMã¨åŒã˜ labels ã‚’æŒ‡å®šï¼‹æœªå‡ºç¾ã‚¯ãƒ©ã‚¹ã¯ zero_division=0 ã§å®‰å…¨ã«
        report_str = classification_report(
            y_true, y_pred,
            labels=labels_idx,
            target_names=class_names,
            digits=4,
            zero_division=0
        )
        report_path = self.results_dir / "classification_report.txt"
        report_path.write_text(report_str)
        
        print("\nClassification Report:")
        print(report_str)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’W&Bã«ä¿å­˜
        self.wandb_run.save(str(report_path), base_path=self.wandb_run.dir)
        print("[Visualizer] Logged classification report to W&B.")
